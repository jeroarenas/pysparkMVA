{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MVA toolbox**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Data reading and preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to load the MNIST data as LabelPoint elements.\n",
    "\n",
    "Note: we have explicitely defined the number of features to be sure that the sparse feature vectors are created with all the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "data = MLUtils.loadLibSVMFile(sc, \"file:///export/usuarios_ml4ds/vanessa/codigo/mnist\", numFeatures= 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data normalization**\n",
    "\n",
    "Now, let's normalize the data:\n",
    "1. Remove the mean of each feature (we have to center the input data, but this is not efficient for sparse data)\n",
    "2. Reescale each feature to make them have unitary standard deviation\n",
    "\n",
    "Prepare output variable:\n",
    "1. Convert $Y$ to 1 vs all codification\n",
    "2. Normalize Y (remove means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create two new RDD by extracting the labels and features of the data\n",
    "label = data.map(lambda x: x.label)\n",
    "features = data.map(lambda x: Vectors.dense(x.features.toArray()))\n",
    "\n",
    "# Define the StandardScaler() object and fit it with the data features \n",
    "scaler = StandardScaler(withMean=True, withStd=True).fit(features)\n",
    "\n",
    "# Normalize the data features\n",
    "features_norm = scaler.transform(features)\n",
    "\n",
    "# Codify labels\n",
    "set_classes = label.distinct().collect()\n",
    "#codedLabel = label.map(lambda x: (x, label_binarize([x], classes=set_classes)))\n",
    "codedLabel = label.map(lambda x: label_binarize([x], classes=set_classes))\n",
    "\n",
    "# Normalize coded labes\n",
    "scaler_labels = StandardScaler(withMean=True, withStd=False).fit(codedLabel)\n",
    "codedLabel_norm = scaler_labels.transform(codedLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new RDD of LabeledPoint data using the normalized features\n",
    "LabeledPointMulticlass = Row(\"label\", \"codedLabel\", \"features\")\n",
    "# 1. Construct a RDD of tuples (label, featatures): check zip() method of RDD objects\n",
    "data_norm = label.zip(codedLabel_norm).zip(features_norm)\n",
    "# 2. Create the label point object\n",
    "data_LP = data_norm.map(lambda x: LabeledPointMulticlass(x[0][0], x[0][1], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create training and validation partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[28] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create trainign and validation partitions\n",
    "(trainingData, valData) = data_LP.randomSplit([0.4, 0.6], seed=0)\n",
    "\n",
    "# Our learning algorithms will make several passes over these datasets, so letâ€™s cache these RDD in memory\n",
    "trainingData.cache()\n",
    "valData.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Can we create a kernel matrix?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import numpy as np\n",
    "\n",
    "trainingData_aux=trainingData.zipWithIndex()\n",
    "K_training = trainingData_aux.cartesian(trainingData_aux)\\\n",
    "            .map(lambda (x1,x2): (x1[1], (x2[1], np.squeeze(euclidean_distances(x1[0].features, x2[0].features, squared=True)))))\\\n",
    "            .groupByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3088, <pyspark.resultiterable.ResultIterable at 0x7f7ec82d1ed0>),\n",
       " (7204, <pyspark.resultiterable.ResultIterable at 0x7f7ecc805fd0>),\n",
       " (19496, <pyspark.resultiterable.ResultIterable at 0x7f7ecc805e90>),\n",
       " (18480, <pyspark.resultiterable.ResultIterable at 0x7f7eca14d690>),\n",
       " (13576, <pyspark.resultiterable.ResultIterable at 0x7f7eca14d650>)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_training.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "import numpy as np\n",
    "\n",
    "M = len(set_classes)\n",
    "R = M -1 # Number of projection vectors\n",
    "\n",
    "# Precompute uselful matrixes\n",
    "# Cyx (MxD)!!!!!!\n",
    "Cyx = trainingData.map(lambda x : np.dot(x.codedLabel[:,np.newaxis],x.features[:,np.newaxis].T)).mean()\n",
    "# Omega (MxM)\n",
    "# PCA y OPLS\n",
    "Omega = np.eye(M) \n",
    "Omega_1 = Omega \n",
    "# CCA  (TODO)\n",
    "#Omega = np.diag(np.power((1./N)*(np.diag(np.dot(Y.T, Y))), -.5))\n",
    "#Omega_1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVA library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create MVA library\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "import numpy as np\n",
    "\n",
    "# This function has to implement Y' = W^T Omega Y\n",
    "def createPseudoY(Y, W, Omega):\n",
    "    return np.squeeze(np.dot(np.dot(W.T, Omega), Y.T))\n",
    "\n",
    "\n",
    "def stepU(W, trainingData, Omega, R, regType='l1', regParam=0.1):\n",
    "    numFeatures = trainingData.first().features.shape[0]\n",
    "    U = np.empty((R,numFeatures))\n",
    "\n",
    "    for r in range(R):\n",
    "        Wr = W[:,r][:,np.newaxis]\n",
    "        PseudoY = trainingData.map (lambda x : createPseudoY(x.codedLabel, Wr, Omega))\n",
    "        Datar = trainingData.zip(PseudoY).map(lambda x: LabeledPoint(x[1], x[0].features))\n",
    "        # Build the model\n",
    "        model = LinearRegressionWithSGD.train(Datar, iterations=100, regType=regType, regParam=regParam, step=0.00000001)\n",
    "        U[r,:] = model.weights\n",
    "    return U\n",
    "\n",
    "def stepW(U, Cyx, Omega, Omega_1):\n",
    "    A = np.dot(Omega, np.dot(Cyx, U.T))  \n",
    "    # Sergio tiene esta linea:  A = np.dot(np.dot(R, np.dot(Cyx, pinvXY)), R)\n",
    "    V, D, V2 = np.linalg.svd(A,full_matrices=False)\n",
    "    W = np.dot(Omega_1,V)\n",
    "    return W\n",
    "\n",
    "def computeMSE(U, W, trainingData):\n",
    "    return trainingData.map(lambda x: np.mean(np.array(x.codedLabel - np.dot(W,np.dot(x.features, U.T)))**2)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# W initialization  (MxR)\n",
    "W = np.eye(M,R)\n",
    "numFeatures = trainingData.first().features.shape[0]\n",
    "U = np.empty((R, numFeatures))\n",
    "# Compute MSE \n",
    "print computeMSE(U, W, trainingData)\n",
    "for iter in range(4):\n",
    "\n",
    "    # Run the iterative process\n",
    "    U = stepU(W, trainingData, Omega, R, regType='l1', regParam=0.05)\n",
    "    W = stepW(U, Cyx, Omega, Omega_1)\n",
    "    print computeMSE(U, W, trainingData)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT the eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "def plot_data(images, h, w, n_row=1, n_col=10):\n",
    "    \"\"\"Plots the set of images provided in images\n",
    "\n",
    "    Args:\n",
    "        images (list of sparse vectors or numpy arrays): list of images where each image contains the \n",
    "            features corresponding to the pixels of an image.  \n",
    "        h: heigth of the image (in number of pixels).\n",
    "        w: width of the image (in number of pixels).\n",
    "        n_row: Number of rows to use when plotting all the images\n",
    "        n_col: Number of columns to use when plotting all the images\n",
    "\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(n_row, n_col, i + 1)      \n",
    "        try:\n",
    "            img = images[i].toArray()\n",
    "        except:\n",
    "            img = images[i]\n",
    "            \n",
    "        plt.imshow(img.reshape((h, w)), cmap=plt.cm.jet)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "h= 28\n",
    "w =28\n",
    "plot_data(U,h,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* Actualizar MLLIB regresor a Spark 2.0\n",
    "* Comprobar con PCA\n",
    "* Comprobar (hacer Omega) para CCA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
